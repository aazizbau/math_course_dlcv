{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 9 — \"Optimization Pathologies: Vanishing & Exploding Gradients\"\n",
        "\n",
        "Gradients flowing through many layers multiply Jacobians. Slightly shrinking (<1) or amplifying (>1) factors compound exponentially, causing gradients to vanish or explode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Each layer behaves like a scaling tunnel for gradients.\n",
        "- Multiplying many Jacobians amplifies small deviations.\n",
        "- Deep stacks without architectural tricks cause gradient death or explosion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Reason\n",
        "\n",
        "`g_N = a^N g_0` shows scalar behavior; |a|<1 → vanishing, |a|>1 → exploding. Matrix version depends on largest singular value of Jacobian products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Simulation — Scalar Multiplication & Jacobian Norms\n",
        "\n",
        "`days/day09/code/gradient_pathologies.py` simulates gradient evolution and random Jacobian norms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day09.code.gradient_pathologies import GradientEvolution, jacobian_singular_values\n",
        "\n",
        "values = GradientEvolution(factors=[0.7, 1.0, 1.3], steps=30).simulate()\n",
        "for a, traj in values.items():\n",
        "    print(f\"a={a}: final gradient = {traj[-1]:.4e}\")\n",
        "\n",
        "norms = jacobian_singular_values(seed=42)\n",
        "print('Average singular value (He init):', np.mean(norms))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization — Gradient Evolution Animation\n",
        "\n",
        "`days/day09/code/visualizations.py` animates vanishing/stable/exploding cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from days.day09.code.visualizations import anim_gradient_evolution\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    gif = anim_gradient_evolution()\n",
        "    print('Saved animation →', gif)\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate Day 9 GIFs in days/day09/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. When & Why These Pathologies Occur\n",
        "\n",
        "- **Vanishing**: sigmoid/tanh, deep plain nets, small weights, RNNs across long sequences.\n",
        "- **Exploding**: large weights, big learning rates, long RNN sequences, Jacobian singular values > 1.\n",
        "- **Effects**: early layers stop learning (vanishing) or training becomes unstable (exploding)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Modern Solutions\n",
        "\n",
        "1. ReLU/variants to prevent saturation.\n",
        "2. BatchNorm/LayerNorm to stabilize activations.\n",
        "3. Residual connections: `J = I + J_f` preserves gradients.\n",
        "4. Proper initialization (Xavier/He) to keep singular values ≈ 1.\n",
        "5. Gradient clipping for explosions.\n",
        "6. LSTMs/GRUs and attention scaling in sequence models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Mini Exercises\n",
        "\n",
        "1. Simulate different factors (0.5, 0.9, 1.05, 1.5).\n",
        "2. Replace ReLU with sigmoid in a toy MLP and inspect gradient norms.\n",
        "3. Visualize singular value distributions for random weight matrices.\n",
        "4. Implement gradient clipping in the exploding simulation.\n",
        "5. Build a multi-layer linear network and measure gradient norms vs depth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "| Concept | Meaning |\n",
        "| --- | --- |\n",
        "| Vanishing gradients | Products of <1 singular values kill gradient flow. |\n",
        "| Exploding gradients | Products >1 cause instability. |\n",
        "| Jacobian products | Root cause; gradient = chain of local linear maps. |\n",
        "| Solutions | ReLU, normalization, residuals, proper init, clipping, gated RNNs. |\n",
        "\n",
        "> Gradient flow is the bloodstream of learning—keep it within healthy ranges."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 20 — \"Modern CNN Architectures: VGG → ResNet → EfficientNet → ConvNeXt\"\n",
        "\n",
        "CNNs evolved by solving concrete problems: VGG proved depth matters, ResNet fixed gradients, EfficientNet scaled efficiently, ConvNeXt modernized CNN design with Transformer lessons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. VGG — Depth as Power\n",
        "\n",
        "- Stack many 3×3 convs (stride 1, same padding) + max pooling.\n",
        "- Two 3×3 ≈ 5×5; three 3×3 ≈ 7×7 receptive fields.\n",
        "- Limitations: huge parameter count, vanishing gradients, slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ResNet — Residual Learning\n",
        "\n",
        "- Learn `H(x) = F(x) + x` to keep gradients alive.\n",
        "- Backward: `dL/dx = dL/dy * (I + dF/dx)` → gradients bypass layers when needed.\n",
        "- Enables 50/101/152-layer networks, smooths loss landscape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. EfficientNet — Compound Scaling\n",
        "\n",
        "- Scale depth/width/resolution together using α,β,γ (subject to `α ⋅ β^2 ⋅ γ^2 ≈ 2`).\n",
        "- Uses depthwise separable convs, SE blocks, Swish activation.\n",
        "- Best accuracy/parameter ratio across mobile + server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ConvNeXt — Transformer-Inspired CNN\n",
        "\n",
        "- Borrow ViT design ideas (GELU, LayerNorm, large kernels, clean stages).\n",
        "- Keeps CNN locality while reaching ViT-level accuracy.\n",
        "- Simplified blocks scale better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Python — Architecture Summary\n",
        "\n",
        "`days/day20/code/architecture_summary.py` prints key ideas for each architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG: Stack many 3x3 convs to go deep → solves Need for depth\n",
            "ResNet: Skip connections with residual blocks → solves Vanishing gradients\n",
            "EfficientNet: Compound scaling of depth/width/resolution → solves Inefficient scaling\n",
            "ConvNeXt: Transformer-inspired CNN redesign → solves Outdated CNN design\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day20.code.architecture_summary import ARCHS\n",
        "\n",
        "for arch in ARCHS:\n",
        "    print(f\"{arch.name}: {arch.key_idea} → solves {arch.solves}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization — Parameter Comparison\n",
        "\n",
        "`days/day20/code/visualizations.py` plots sample parameter counts for VGG/ResNet/EfficientNet/ConvNeXt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RUN_ANIMATIONS = True to regenerate Day 20 figures in days/day20/outputs/.\n"
          ]
        }
      ],
      "source": [
        "from days.day20.code.visualizations import plot_param_comparison\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    print('Saved parameter plot →', plot_param_comparison())\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate Day 20 figures in days/day20/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Architecture Comparison (Big Picture)\n",
        "\n",
        "| Model | Key Idea | Solves |\n",
        "| --- | --- | --- |\n",
        "| VGG | Go deeper with stacks of 3×3 convs | Limited representation |\n",
        "| ResNet | Residual skip connections | Vanishing gradients |\n",
        "| EfficientNet | Compound scaling of depth/width/resolution | Inefficient scaling |\n",
        "| ConvNeXt | Transformer-inspired CNN redesign | Outdated CNN design |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Mini Exercises\n",
        "\n",
        "1. Compare feature maps from VGG vs ResNet at equal depth.\n",
        "2. Remove skip connections in ResNet and observe training failure.\n",
        "3. Train EfficientNet-B0 vs ResNet-50 on a small dataset; compare accuracy/efficiency.\n",
        "4. Replace ReLU+BN with GELU+LN in a CNN block; test optimization stability.\n",
        "5. Visualize receptive fields in ConvNeXt blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Takeaways\n",
        "\n",
        "- VGG: depth is powerful but parameter-heavy.\n",
        "- ResNet: identity paths solved gradient issues.\n",
        "- EfficientNet: balanced scaling beats brute force.\n",
        "- ConvNeXt: CNNs can be modernized to match Transformers.\n",
        "- Architecture design combines geometry, gradients, and optimization lessons.\n",
        "\n",
        "> Every great architecture responds to a limitation of the previous one."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

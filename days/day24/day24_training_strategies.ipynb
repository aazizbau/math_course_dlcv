{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 24 — \"Training Strategies for Dense Prediction (LR Schedules, Augmentation, Curriculum)\"\n",
        "\n",
        "Dense prediction needs careful optimization, spatially consistent augmentation, and progressive difficulty. Strategy shapes how gradients explore the loss landscape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using repo root: /media/abdul-aziz/sdb7/masters_research/math_course_dlcv\n"
          ]
        }
      ],
      "source": [
        "# Ensure repo root is on sys.path for local imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if not (repo_root / \"days\").exists():\n",
        "    for parent in Path.cwd().resolve().parents:\n",
        "        if (parent / \"days\").exists():\n",
        "            repo_root = parent\n",
        "            break\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print(f\"Using repo root: {repo_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Dense prediction is sensitive to learning rate, data diversity, and training order.\n",
        "- LR schedules steer optimization from coarse structure to fine boundaries.\n",
        "- Augmentations must preserve pixel alignment.\n",
        "- Curriculum learning stabilizes training on non-convex landscapes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Learning-Rate Schedules\n",
        "\n",
        "- **Step decay**: abrupt drops to refine later stages.\n",
        "- **Cosine annealing**: smooth decay for stable refinement.\n",
        "- **One-cycle**: fast exploration followed by aggressive refinement.\n",
        "- **Warmup**: avoids early instability in deep or BN-heavy networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Warmup\n",
        "\n",
        "Warmup increases LR gradually to prevent early divergence. It is especially helpful with large batches, mixed precision, or transformer-like backbones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Augmentation (Geometry Matters)\n",
        "\n",
        "Spatial augmentations must be applied **to both images and masks**:\n",
        "\n",
        "- flips, crops, rotation, scale\n",
        "- elastic deformation for medical imagery\n",
        "\n",
        "Photometric augmentations apply **only to images** (color jitter, noise, blur).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Curriculum Learning\n",
        "\n",
        "- **Sample curriculum**: start with easy cases, introduce hard scenes later.\n",
        "- **Resolution curriculum**: train low-res first, then higher-res for detail.\n",
        "- **Loss curriculum**: start with BCE, then add Dice/IoU for shape fidelity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Batch Size, Accumulation, AMP\n",
        "\n",
        "- Dense prediction uses large images and small batches.\n",
        "- Use gradient accumulation to simulate larger batches.\n",
        "- Mixed precision (AMP) improves speed and stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Python — LR Schedule Demos\n",
        "\n",
        "`days/day24/code/training_strategies.py` provides simple schedule generators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: [SchedulePoint(step=0, lr=0.001), SchedulePoint(step=1, lr=0.001), SchedulePoint(step=2, lr=0.001), SchedulePoint(step=3, lr=0.001), SchedulePoint(step=4, lr=0.001)]\n",
            "Cosine: [SchedulePoint(step=0, lr=0.001), SchedulePoint(step=1, lr=0.0008536998372026805), SchedulePoint(step=2, lr=0.0005005000000000001), SchedulePoint(step=3, lr=0.00014730016279731955), SchedulePoint(step=4, lr=1e-06)]\n",
            "One-cycle: [SchedulePoint(step=0, lr=1e-05), SchedulePoint(step=1, lr=0.001), SchedulePoint(step=2, lr=0.00067), SchedulePoint(step=3, lr=0.00034), SchedulePoint(step=4, lr=1.0000000000000026e-05)]\n",
            "Warmup: [SchedulePoint(step=0, lr=0.0003333333333333333), SchedulePoint(step=1, lr=0.0006666666666666666), SchedulePoint(step=2, lr=0.001), SchedulePoint(step=3, lr=0.001), SchedulePoint(step=4, lr=0.001)]\n"
          ]
        }
      ],
      "source": [
        "from days.day24.code.training_strategies import (\n",
        "    step_decay,\n",
        "    cosine_annealing,\n",
        "    one_cycle,\n",
        "    warmup_linear,\n",
        ")\n",
        "\n",
        "print(\"Step:\", step_decay(1e-3, 0.1, 20, 5))\n",
        "print(\"Cosine:\", cosine_annealing(1e-3, 1e-6, 5))\n",
        "print(\"One-cycle:\", one_cycle(1e-3, 1e-5, 5))\n",
        "print(\"Warmup:\", warmup_linear(1e-3, 3, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualization — LR Schedule Comparison\n",
        "\n",
        "`days/day24/code/visualizations.py` plots step, cosine, one-cycle, and warmup curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RUN_FIGURES = True to regenerate Day 24 figures inside days/day24/outputs/.\n"
          ]
        }
      ],
      "source": [
        "from days.day24.code.visualizations import plot_lr_schedules\n",
        "\n",
        "RUN_FIGURES = False\n",
        "\n",
        "if RUN_FIGURES:\n",
        "    plot_lr_schedules()\n",
        "else:\n",
        "    print(\"Set RUN_FIGURES = True to regenerate Day 24 figures inside days/day24/outputs/.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Default Training Recipe (UNet/FPN)\n",
        "\n",
        "- Optimizer: AdamW\n",
        "- LR: 3e-4\n",
        "- Scheduler: Cosine + warmup\n",
        "- Loss: BCE + Dice\n",
        "- Augmentation: flip + crop + scale\n",
        "- AMP: on\n",
        "- Batch size: as large as possible (or accumulate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Mini Exercises\n",
        "\n",
        "1. Compare fixed LR vs cosine schedule on the same model.\n",
        "2. Disable augmentation and track overfitting.\n",
        "3. Try warmup vs no warmup and compare early stability.\n",
        "4. Train low-res then high-res (resolution curriculum).\n",
        "5. Compare one-cycle vs cosine on your dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Takeaways\n",
        "\n",
        "- Training strategy shapes how gradients explore geometry.\n",
        "- LR schedules control stability and refinement.\n",
        "- Augmentation teaches invariance and robustness.\n",
        "- Curriculum learning smooths non-convex optimization.\n",
        "- Good training beats fancy architecture.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 3 \u2014 \"Rolling Down the Hill\": Gradient Descent and Optimization Intuition\n\nTraining a neural network is like rolling a marble through fog: gradients tell you which way is downhill, even when you cannot see the full landscape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n\n- The loss surface is a mountain; weights are the marble's coordinates.\n- Gradients are the local slopes that guide you toward lower loss.\n- Large slopes accelerate movement; gentle slopes slow progress.\n- Each update nudges the model downhill, sculpting it into a better predictor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Story \u2014 The Downhill Equation\n\n| Concept | Formula | Intuitive Meaning |\n| --- | --- | --- |\n| Loss function | `L(\u03b8)` | Landscape we minimize (prediction error). |\n| Gradient | `\u2207_\u03b8 L = [\u2202L/\u2202\u03b8\u2081, \u2202L/\u2202\u03b8\u2082, \u2026]` | Local slope / direction of steepest ascent. |\n| Update rule | `\u03b8_{t+1} = \u03b8_t - \u03b7 \u2207_\u03b8 L(\u03b8_t)` | Step opposite the gradient to descend. |\n| Learning rate | `\u03b7` | Step size controlling how far each update travels. |\n| Optimum | `\u2207_\u03b8 L = 0` | Flat spot / valley bottom. |\n\nExample bowl: `L(x, y) = 0.5 (x\u00b2 + y\u00b2)` \u21d2 `\u2207L = [x, y]` and each iteration shrinks both coordinates by `(1 - \u03b7)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Implementation \u2014 The Math as Code\n\n`days/day03/code/gradient_descent.py` encapsulates the quadratic bowl, its gradient, and a reusable runner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\n\ndef find_repo_root(marker: str = \"days\") -> Path:\n    path = Path.cwd()\n    while path != path.parent:\n        if (path / marker).exists():\n            return path\n        path = path.parent\n    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n\nREPO_ROOT = find_repo_root()\nif str(REPO_ROOT) not in sys.path:\n    sys.path.append(str(REPO_ROOT))\n\nfrom days.day03.code.gradient_descent import GradientDescentRunner, QuadraticBowl\n\nrunner = GradientDescentRunner(bowl=QuadraticBowl(), lr=0.2, steps=10)\npath = runner.run([2.5, -2.0])\nprint(path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The coordinates shrink toward `(0, 0)`\u2014our valley minimum. Increasing the learning rate moves faster but risks oscillation; decreasing it slows convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization \u2014 Watching the Marble Roll Downhill\n\n`days/day03/code/visualizations.py` generates:\n1. Gradient descent trajectory on a contour plot.\n2. Learning-rate comparison (slow vs. ideal vs. oscillatory).\n3. Gradient field quiver plot (optional still image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from days.day03.code.visualizations import (\n    anim_gradient_descent,\n    anim_learning_rates,\n    render_gradient_field,\n)\n\nRUN_ANIMATIONS = False\n\nif RUN_ANIMATIONS:\n    assets = [\n        anim_gradient_descent(),\n        anim_learning_rates(),\n        render_gradient_field(),\n    ]\n    for asset in assets:\n        print(f\"Saved asset \u2192 {asset}\")\nelse:\n    print('Set RUN_ANIMATIONS = True to regenerate GIFs/PNGs in days/day03/outputs/.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deep Learning & Computer Vision Connections\n\n| Concept | In Practice |\n| --- | --- |\n| Gradient descent | Core learning rule for neural nets (SGD, Adam build on it). |\n| Learning rate | Governs stability vs. speed of convergence. |\n| Loss surface | Encodes model quality; diagnostics via loss landscapes. |\n| Gradient | Guides saliency maps and adversarial example crafting. |\n| Local minima | Different solutions with similar performance; initialization matters. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Mini Exercises\n\n1. Sweep `\u03b7` from `0.05` \u2192 `0.5` and observe when oscillations/divergence appear.\n2. Modify `bowl_loss` to include stronger cross terms (e.g., `+ 2xy`) to twist the valley.\n3. Start from multiple initial points and plot their trajectories on the same contour.\n4. Add random Gaussian noise to the gradient to mimic stochastic updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n\n| Concept | Meaning |\n| --- | --- |\n| Gradient | Local direction of steepest ascent (flip sign to descend). |\n| Descent step | `-\u03b7 \u2207L` \u2014 the nudge that lowers loss. |\n| Learning rate | Trade-off between speed and stability. |\n| Loss surface | High-dimensional landscape encoding model performance. |\n| Convergence | Landing in a flat, low-loss region \u21d2 learned parameters. |\n\n> Optimization is not magic\u2014it\u2019s just a marble following gravity in a high-dimensional valley."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
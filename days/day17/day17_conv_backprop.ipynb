{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 17 — \"Backpropagation Through Convolutions (Computational Graph + Intuition)\"\n",
        "\n",
        "A convolution kernel is a stamp. Forward pass slides the stamp; backward pass gathers feedback from every imprint to reshape the stamp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Forward: sliding window sums local patches.\n",
        "- Backward: each output pixel sends gradient heat to its patch + shared kernel weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Breakdown\n",
        "\n",
        "`dW[u,v] = ∑ dY[i,j] * x[i+u,j+v]`.\n",
        "`dX = dY * rot180(W)` (full conv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python — Naive Conv Forward/Backward\n",
        "\n",
        "`days/day17/code/conv_backprop.py` implements the functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward output shape: (3, 3)\n",
            "dW shape: (3, 3) dX shape: (5, 5)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day17.code.conv_backprop import conv2d_forward, conv2d_backward\n",
        "\n",
        "x = np.random.randn(5,5)\n",
        "w = np.random.randn(3,3)\n",
        "y = conv2d_forward(x, w)\n",
        "dW, dX = conv2d_backward(x, w, np.ones_like(y))\n",
        "print('Forward output shape:', y.shape)\n",
        "print('dW shape:', dW.shape, 'dX shape:', dX.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization — Gradient Accumulation\n",
        "\n",
        "`days/day17/code/visualizations.py` animates forward windows and weight-gradient buildup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RUN_ANIMATIONS = True to regenerate Day 17 figures in days/day17/outputs/.\n"
          ]
        }
      ],
      "source": [
        "from days.day17.code.visualizations import anim_conv_backprop\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    gif_path = anim_conv_backprop()\n",
        "    print('Saved animation →', gif_path)\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate Day 17 figures in days/day17/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradient Flow Insight\n",
        "\n",
        "- Weight sharing ⇒ gradients sum across positions.\n",
        "- Input gradients use flipped kernels.\n",
        "- Efficient GPU kernels reuse convolution logic for backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Mini Exercises\n",
        "\n",
        "1. Modify stride/dilation and recompute gradients.\n",
        "2. Extend to multi-channel convs.\n",
        "3. Compare naive gradients with PyTorch autograd.\n",
        "4. Visualize weight gradient heatmaps.\n",
        "5. Implement backprop for dilated conv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key Takeaways\n",
        "\n",
        "| Point | Meaning |\n",
        "| --- | --- |\n",
        "| Backprop slides windows in reverse | each output contributes to weights & inputs. |\n",
        "| Weight gradients accumulate | due to shared kernels. |\n",
        "| Input gradients = conv with flipped kernels | chain rule in action. |\n",
        "| Understanding conv backprop | essential for designing CNN variants. |\n",
        "\n",
        "> Convolution backprop is a choreography of gradients—each location reshapes the shared kernel."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 11 — \"Normalization Layers: BatchNorm, LayerNorm & Gradient Stability\"\n",
        "\n",
        "Normalization resets activation distributions so each layer learns on stable, predictable ranges. That keeps gradients flowing and training fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Deep nets see their activation scales drift; gradients become unstable.\n",
        "- Normalization recenters/rescales before each layer, like leveling the ground before building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. What Normalization Does\n",
        "\n",
        "- Mean → 0, variance → 1.\n",
        "- Stabilizes gradient flow, smooths the loss surface, allows larger learning rates, reduces sensitivity to initialization, and makes deeper models trainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Batch Normalization\n",
        "\n",
        "`mu = mean(x, batch)`, `var = var(x, batch)`, `xhat = (x-mu)/sqrt(var+eps)`, `y = gamma*xhat + beta`. Works best for CNNs; suffers when batch sizes vary or are tiny (RNNs/Transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Layer Normalization\n",
        "\n",
        "Normalize across features per sample: batch-size agnostic and perfect for Transformers, LSTMs, and attention blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Python Implementation — BatchNorm vs LayerNorm\n",
        "\n",
        "`days/day11/code/normalization.py` implements forward passes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel '.venv (Python 3.12.3)' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day11.code.normalization import batchnorm_forward, layernorm_forward\n",
        "\n",
        "x = np.random.randn(4, 5)\n",
        "print('BatchNorm:\\n', batchnorm_forward(x))\n",
        "print('LayerNorm:\\n', layernorm_forward(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization — Distribution Stabilization\n",
        "\n",
        "`days/day11/code/visualizations.py` animates drifting activations vs BatchNorm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from days.day11.code.visualizations import anim_batchnorm_distribution\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    gif = anim_batchnorm_distribution()\n",
        "    print('Saved animation →', gif)\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate Day 11 GIFs in days/day11/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. How Normalization Helps Gradients\n",
        "\n",
        "- Prevents exploding activations (variance ≈1).\n",
        "- Keeps gradients in sensitive regions (less vanishing).\n",
        "- Smooths curvature, enables larger learning rates, reduces internal covariate shift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Which Architecture Uses Which Normalizer\n",
        "\n",
        "| Architecture | Normalization |\n",
        "| --- | --- |\n",
        "| ResNet / MobileNet | BatchNorm |\n",
        "| Transformers / GPT / ViT | LayerNorm / RMSNorm |\n",
        "| Style transfer | InstanceNorm |\n",
        "| 3D CNNs / tiny batches | GroupNorm |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Mini Exercises\n",
        "\n",
        "1. Train a small CNN with/without BatchNorm; compare gradient norms.\n",
        "2. Shrink batch size (32→2→1) to see BatchNorm instability.\n",
        "3. Swap BatchNorm for LayerNorm in a CNN and measure accuracy.\n",
        "4. Try GroupNorm/InstanceNorm when batches are tiny.\n",
        "5. Compare gradient norms in a deep MLP with vs without normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Takeaways\n",
        "\n",
        "| Point | Meaning |\n",
        "| --- | --- |\n",
        "| Normalization stabilizes optimization | Keeps activations/gradients in safe ranges. |\n",
        "| BatchNorm | Batch-wise stats, great for CNNs. |\n",
        "| LayerNorm/RMSNorm | Feature-wise stats, great for Transformers/RNNs. |\n",
        "| Internal covariate shift | Reduced when distributions stay fixed. |\n",
        "| Enables depth/speed | Higher learning rates & deeper nets. |\n",
        "\n",
        "> Normalization reshapes the terrain of learning—turning jagged mountains into smooth hills."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

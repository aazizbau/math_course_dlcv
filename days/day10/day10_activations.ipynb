{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 10 \u2014 \"Activation Functions: Geometry, Derivatives & Optimization Effects\"\n\nActivation functions are the folds that give neural networks nonlinear expressivity. They sculpt geometry and control gradient flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n\n- Linear layers alone collapse to a single linear map.\n- Activations bend space, creating folds, thresholds, and curved regions.\n- Their derivatives determine how gradients propagate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Popular Activations\n\n| Function | Formula | Derivative | Geometry/Notes |\n| --- | --- | --- | --- |\n| Sigmoid | `\u03c3(x) = 1/(1+e^{-x})` | `\u03c3(x)(1-\u03c3(x))` | S-curve (0\u21921); saturates, vanishing grads. |\n| Tanh | `(e^x-e^{-x})/(e^x+e^{-x})` | `1 - tanh^2(x)` | Zero-centered S-curve, still saturates. |\n| ReLU | `max(0,x)` | `1 (x>0) else 0` | Piecewise linear; strong gradients but dead units. |\n| Leaky ReLU | `x` if `x>0` else `\u03b1x` | `1` or `\u03b1` | Fixes dead ReLU. |\n| GELU | `0.5x(1+tanh(\u221a(2/\u03c0)(x+0.044715x^3)))` | smooth | Used in Transformers; smooth gradient flow. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Implementation \u2014 Activation Utilities\n\n`days/day10/code/activations.py` bundles functions and derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\n\n\ndef find_repo_root(marker: str = \"days\") -> Path:\n    path = Path.cwd()\n    while path != path.parent:\n        if (path / marker).exists():\n            return path\n        path = path.parent\n    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n\nREPO_ROOT = find_repo_root()\nif str(REPO_ROOT) not in sys.path:\n    sys.path.append(str(REPO_ROOT))\n\nfrom days.day10.code.activations import build_activations\n\nacts = build_activations()\nx = np.linspace(-5, 5, 5)\nfor name, act in acts.items():\n    print(name, act.fn(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization \u2014 Curves & Derivatives\n\n`days/day10/code/visualizations.py` plots activations and animates their derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from days.day10.code.visualizations import plot_activations, anim_activation_derivatives\n\nRUN_ANIMATIONS = False\n\nif RUN_ANIMATIONS:\n    x = np.linspace(-5, 5, 400)\n    path_curve = plot_activations(x)\n    path_gif = anim_activation_derivatives(x)\n    print('Saved assets \u2192', path_curve, path_gif)\nelse:\n    print('Set RUN_ANIMATIONS = True to regenerate Day 10 figures in days/day10/outputs/.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimization Effects\n\n- Gradient flow: ReLU/GELU keep gradients alive; sigmoid/tanh shrink them.\n- Loss landscape: smooth activations (GELU/Swish) lead to smoother surfaces.\n- Architecture choices: CNNs (ReLU), Transformers (GELU), RNN gates (sigmoid/tanh)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Mini Exercises\n\n1. Compare ReLU vs LeakyReLU derivatives.\n2. Train a small MLP with Sigmoid vs ReLU; inspect gradient norms.\n3. Swap GELU into a CNN and measure training speed.\n4. Visualize gradient norms across layers for different activations.\n5. Add BatchNorm to tanh networks and observe stability improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n\n| Point | Meaning |\n| --- | --- |\n| Activations bend space | Enable complex decision boundaries. |\n| Derivatives control gradient flow | Good activations preserve signal. |\n| ReLU revolutionized deep nets | No saturation on positive side. |\n| GELU/Swish | Smooth, modern choices (LLMs, ViTs). |\n| Architecture-specific choices | Use the activation that fits gradient needs. |\n\n> Activations are the soul of deep learning\u2014they shape the geometry of learning itself."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
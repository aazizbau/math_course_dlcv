{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 36 — \"Limits, Continuity & Why Gradients Exist\"\n",
        "\n",
        "Gradients exist because the world changes smoothly. Limits and continuity are the mathematical guarantees behind learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [],
      "source": [
        "# Ensure repo root is on sys.path for local imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if not (repo_root / \"days\").exists():\n",
        "    for parent in Path.cwd().resolve().parents:\n",
        "        if (parent / \"days\").exists():\n",
        "            repo_root = parent\n",
        "            break\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print(f\"Using repo root: {repo_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "Gradient descent assumes the loss surface changes smoothly. Without limits and continuity, gradients are meaningless.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Limits\n",
        "\n",
        "Limits formalize what happens as changes become tiny. If a function doesn’t approach a stable value, gradients cannot exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Continuity\n",
        "\n",
        "A function is continuous when the value at a point matches the value approached from nearby points. No jumps, no teleports.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Derivative as a Limit\n",
        "\n",
        "The derivative is defined by a limit of difference quotients. It is the local slope that gradients use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Python — Numerical Derivatives\n",
        "\n",
        "`days/day36/code/limits_continuity.py` approximates derivatives and shows |x| at 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [],
      "source": [
        "from days.day36.code.limits_continuity import derivative_approx, f_square, f_abs\n",
        "\n",
        "x = 2.0\n",
        "h = 1e-5\n",
        "print(\"Approx derivative:\", derivative_approx(f_square, x, h))\n",
        "print(\"True derivative:\", 2 * x)\n",
        "\n",
        "for h in [1e-1, 1e-2, 1e-3, 1e-4]:\n",
        "    print(\"|x| derivative at 0 with h=\", h, derivative_approx(f_abs, 0.0, h))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization — Difference Quotient\n",
        "\n",
        "`days/day36/code/visualizations.py` shows how the difference quotient converges and why |x| is non-smooth at 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [],
      "source": [
        "from days.day36.code.visualizations import plot_difference_quotient, plot_abs_kink\n",
        "\n",
        "RUN_FIGURES = False\n",
        "\n",
        "if RUN_FIGURES:\n",
        "    plot_difference_quotient()\n",
        "    plot_abs_kink()\n",
        "else:\n",
        "    print(\"Set RUN_FIGURES = True to regenerate Day 36 figures inside days/day36/outputs/.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Smooth vs Non-Smooth\n",
        "\n",
        "Deep learning prefers continuous functions and differentiable (almost everywhere) activations. ReLU works because its kink is rare.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Mini Exercises\n",
        "\n",
        "1. Approximate derivative of |x| near zero.\n",
        "2. Compare smooth vs step loss in optimization.\n",
        "3. Visualize local linear approximations for different functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key Takeaways\n",
        "\n",
        "- Limits formalize smooth change.\n",
        "- Continuity avoids jumps.\n",
        "- Gradients exist because functions behave locally linearly.\n",
        "- Non-smoothness must be controlled.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 12 — \"Weight Initialization: Keeping Signal & Gradient Balanced\"\n",
        "\n",
        "Initialization keeps information flowing—without it, deep networks drown in vanishing or exploding signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Forward/backward passes behave like long pipelines.\n",
        "- Poor initialization gradually kills or explodes signals → gradients vanish or blow up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Goal\n",
        "\n",
        "Maintain `Var(x_l+1) ≈ Var(x_l)` and `Var(δ_l) ≈ Var(δ_{l+1})` so neither activations nor gradients collapse/explode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Xavier (Glorot) Initialization\n",
        "\n",
        "- For tanh/sigmoid nets.\n",
        "- `W ∼ N(0, 2/(n_in+n_out))` or uniform variant.\n",
        "- Keeps both forward/backward variance stable for saturating activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. He (Kaiming) Initialization\n",
        "\n",
        "- For ReLU (half activations zero).\n",
        "- `W ∼ N(0, 2/n_in)` or uniform variant.\n",
        "- Compensates for ReLU sparsity so signal stays alive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Python Simulation — Variance Propagation\n",
        "\n",
        "`days/day12/code/initialization.py` provides utilities to inspect variance stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xavier final variance = 0.0231\n",
            "he final variance = 0.2639\n",
            "bad final variance = 1100063550125093781839691019976704.0000\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day12.code.initialization import VarianceSimulator\n",
        "\n",
        "sim = VarianceSimulator(layers=20, width=128, seed=42)\n",
        "for init in (\"xavier\", \"he\", \"bad\"):\n",
        "    variances = sim.run(init)\n",
        "    print(f\"{init} final variance = {variances[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization — Variance Across Layers\n",
        "\n",
        "`days/day12/code/visualizations.py` animates how Xavier/He compare to bad init."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RUN_ANIMATIONS = True to regenerate Day 12 GIFs in days/day12/outputs/.\n"
          ]
        }
      ],
      "source": [
        "from days.day12.code.visualizations import anim_variance_evolution\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    gif = anim_variance_evolution()\n",
        "    print('Saved variance animation →', gif)\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate Day 12 GIFs in days/day12/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Why Initialization Matters\n",
        "\n",
        "- Prevents vanishing/exploding gradients.\n",
        "- Lets deep nets converge quickly; interacts with normalization and learning rate.\n",
        "- Reduces sensitivity to bad hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Initialization by Architecture\n",
        "\n",
        "| Architecture | Initialization |\n",
        "| --- | --- |\n",
        "| ReLU CNNs / ResNets | He (Kaiming) |\n",
        "| tanh/sigmoid MLPs | Xavier |\n",
        "| Transformers | Xavier uniform + RMSNorm |\n",
        "| RNNs | Orthogonal init |\n",
        "| ResNets final BN | Zero-gamma trick |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Mini Exercises\n",
        "\n",
        "1. Replace Xavier with small constants; plot gradient norms across 20 layers.\n",
        "2. Train MNIST with constant init to observe collapse.\n",
        "3. Use orthogonal init for an RNN and compare to random gaussian.\n",
        "4. Compare training curves: He vs Xavier for ReLU nets.\n",
        "5. Visualize eigenvalues of weight matrices for different initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Takeaways\n",
        "\n",
        "| Point | Meaning |\n",
        "| --- | --- |\n",
        "| Initialization controls signal flow | Keeps activations and gradients balanced. |\n",
        "| Xavier | Stable tanh/sigmoid nets. |\n",
        "| He | Lets deep ReLU nets learn. |\n",
        "| Poor init | Causes immediate collapse; no training hack can save it. |\n",
        "| Init + normalization | Foundation of modern deep learning stability. |\n",
        "\n",
        "> Initialization is the first decision that decides whether a network can learn at all."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

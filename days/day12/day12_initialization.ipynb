{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 12 \u2014 \"Weight Initialization: Keeping Signal & Gradient Balanced\"\n\nInitialization keeps information flowing\u2014without it, deep networks drown in vanishing or exploding signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n\n- Forward/backward passes behave like long pipelines.\n- Poor initialization gradually kills or explodes signals \u2192 gradients vanish or blow up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Goal\n\nMaintain `Var(x_l+1) \u2248 Var(x_l)` and `Var(\u03b4_l) \u2248 Var(\u03b4_{l+1})` so neither activations nor gradients collapse/explode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Xavier (Glorot) Initialization\n\n- For tanh/sigmoid nets.\n- `W \u223c N(0, 2/(n_in+n_out))` or uniform variant.\n- Keeps both forward/backward variance stable for saturating activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. He (Kaiming) Initialization\n\n- For ReLU (half activations zero).\n- `W \u223c N(0, 2/n_in)` or uniform variant.\n- Compensates for ReLU sparsity so signal stays alive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Python Simulation \u2014 Variance Propagation\n\n`days/day12/code/initialization.py` provides utilities to inspect variance stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\n\ndef find_repo_root(marker: str = \"days\") -> Path:\n    path = Path.cwd()\n    while path != path.parent:\n        if (path / marker).exists():\n            return path\n        path = path.parent\n    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n\nREPO_ROOT = find_repo_root()\nif str(REPO_ROOT) not in sys.path:\n    sys.path.append(str(REPO_ROOT))\n\nfrom days.day12.code.initialization import VarianceSimulator\n\nsim = VarianceSimulator(layers=20, width=128, seed=42)\nfor init in (\"xavier\", \"he\", \"bad\"):\n    variances = sim.run(init)\n    print(f\"{init} final variance = {variances[-1]:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization \u2014 Variance Across Layers\n\n`days/day12/code/visualizations.py` animates how Xavier/He compare to bad init."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from days.day12.code.visualizations import anim_variance_evolution\n\nRUN_ANIMATIONS = False\n\nif RUN_ANIMATIONS:\n    gif = anim_variance_evolution()\n    print('Saved variance animation \u2192', gif)\nelse:\n    print('Set RUN_ANIMATIONS = True to regenerate Day 12 GIFs in days/day12/outputs/.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Why Initialization Matters\n\n- Prevents vanishing/exploding gradients.\n- Lets deep nets converge quickly; interacts with normalization and learning rate.\n- Reduces sensitivity to bad hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Initialization by Architecture\n\n| Architecture | Initialization |\n| --- | --- |\n| ReLU CNNs / ResNets | He (Kaiming) |\n| tanh/sigmoid MLPs | Xavier |\n| Transformers | Xavier uniform + RMSNorm |\n| RNNs | Orthogonal init |\n| ResNets final BN | Zero-gamma trick |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Mini Exercises\n\n1. Replace Xavier with small constants; plot gradient norms across 20 layers.\n2. Train MNIST with constant init to observe collapse.\n3. Use orthogonal init for an RNN and compare to random gaussian.\n4. Compare training curves: He vs Xavier for ReLU nets.\n5. Visualize eigenvalues of weight matrices for different initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Takeaways\n\n| Point | Meaning |\n| --- | --- |\n| Initialization controls signal flow | Keeps activations and gradients balanced. |\n| Xavier | Stable tanh/sigmoid nets. |\n| He | Lets deep ReLU nets learn. |\n| Poor init | Causes immediate collapse; no training hack can save it. |\n| Init + normalization | Foundation of modern deep learning stability. |\n\n> Initialization is the first decision that decides whether a network can learn at all."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
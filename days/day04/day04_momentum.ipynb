{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4 — \"Momentum and Nesterov\": The Ball with Inertia\n",
        "\n",
        "Momentum gives our optimization marble memory: instead of zigzagging with every gradient, it carries speed through valleys, smoothing the ride toward lower loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Intuition\n",
        "\n",
        "- Vanilla gradient descent = cautious step-by-step walking.\n",
        "- Momentum = sliding downhill with inertia — it remembers consistent downhill directions.\n",
        "- Nesterov momentum peeks ahead before committing, allowing smoother corrections.\n",
        "- Result: faster, stabler training on twisty or noisy loss surfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Story — Adding Memory\n",
        "\n",
        "| Concept | Formula | Meaning |\n",
        "| --- | --- | --- |\n",
        "| Vanilla update | `θ_{t+1} = θ_t - η ∇L(θ_t)` | Move opposite the gradient. |\n",
        "| Momentum velocity | `v_{t+1} = β v_t + (1-β)(-η ∇L(θ_t))` | Weighted average of past gradients. |\n",
        "| Momentum update | `θ_{t+1} = θ_t + v_{t+1}` | Apply the velocity to parameters. |\n",
        "| Nesterov lookahead | `v_{t+1} = β v_t - η ∇L(θ_t + β v_t)` | Measure slope after the momentum step. |\n",
        "| β parameter | `β ∈ [0,1)` | Memory strength (0 = vanilla GD, 0.9 common). |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Implementation — Momentum vs Vanilla\n",
        "\n",
        "Helper functions live in `days/day04/code/momentum_methods.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_repo_root(marker: str = \"days\") -> Path:\n",
        "    path = Path.cwd()\n",
        "    while path != path.parent:\n",
        "        if (path / marker).exists():\n",
        "            return path\n",
        "        path = path.parent\n",
        "    raise RuntimeError(\"Run this notebook from inside the repository tree.\")\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from days.day04.code.momentum_methods import Bowl, OptimizerConfig, gradient_descent, momentum, nesterov\n",
        "\n",
        "bowl = Bowl()\n",
        "config = OptimizerConfig(lr=0.15, beta=0.9, steps=12)\n",
        "init = [2.5, -2.0]\n",
        "\n",
        "gd_path = gradient_descent(init, bowl, lr=config.lr, steps=config.steps)\n",
        "mom_path = momentum(init, bowl, config)\n",
        "nag_path = nesterov(init, bowl, config)\n",
        "\n",
        "print(\"GD tail:\", gd_path[-3:])\n",
        "print(\"Momentum tail:\", mom_path[-3:])\n",
        "print(\"Nesterov tail:\", nag_path[-3:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Momentum (and especially Nesterov) reaches near the origin faster and with less zigzagging — greater distances covered per step because velocity carries past gradients forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization — The Marble with Inertia\n",
        "\n",
        "`days/day04/code/visualizations.py` animates: (1) GD vs momentum, (2) β sweeps, (3) momentum vs Nesterov."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from days.day04.code.visualizations import (\n",
        "    anim_gd_vs_momentum,\n",
        "    anim_momentum_beta,\n",
        "    anim_momentum_vs_nesterov,\n",
        ")\n",
        "\n",
        "RUN_ANIMATIONS = False\n",
        "\n",
        "if RUN_ANIMATIONS:\n",
        "    assets = [\n",
        "        anim_gd_vs_momentum(),\n",
        "        anim_momentum_beta(),\n",
        "        anim_momentum_vs_nesterov(),\n",
        "    ]\n",
        "    for asset in assets:\n",
        "        print(f\"Saved asset → {asset}\")\n",
        "else:\n",
        "    print('Set RUN_ANIMATIONS = True to regenerate GIFs in days/day04/outputs/.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deep Learning & Computer Vision Connections\n",
        "\n",
        "| Concept | Real-Life Usage |\n",
        "| --- | --- |\n",
        "| Momentum | Default in SGD for CNNs; stabilizes noisy gradients. |\n",
        "| Nesterov | Used in many CV pipelines (ResNet, segmentation) for smoother convergence. |\n",
        "        | β parameter | Typically 0.9/0.99 — controls memory length. |\n",
        "        | Effect on loss | Faster convergence, less zigzag, better conditioning. |\n",
        "        | Analogy | Heavy ball rolling through valleys, ignoring small bumps. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Mini Exercises\n",
        "\n",
        "1. Try β = 0, 0.5, 0.9, 0.99; note overshoot vs sluggishness.\n",
        "2. Inject random noise into gradients to mimic SGD — watch momentum filter it.\n",
        "3. Modify the surface to have multiple minima and study whether momentum jumps shallow pits.\n",
        "4. Extend the code to animate Nesterov vs vanilla momentum with different β."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n",
        "\n",
        "| Concept | Meaning |\n",
        "| --- | --- |\n",
        "| Momentum | Adds memory — move in the averaged gradient direction. |\n",
        "| β (beta) | Memory strength; high β smooths updates (common 0.9). |\n",
        "| Nesterov | Look ahead before pushing, leading to refined corrections. |\n",
        "| Benefit | Faster and more stable convergence. |\n",
        "| Analogy | A ball with inertia rolling through curved valleys. |\n",
        "\n",
        "> Momentum helps optimization remember its direction — it stops hesitating and starts flowing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

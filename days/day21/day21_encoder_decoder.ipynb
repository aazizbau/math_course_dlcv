{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dacb9eeb",
      "metadata": {},
      "source": [
        "\n",
        "# Day 21 — \"UNet, FPN & Encoder–Decoder Architectures for Dense Prediction\"\n",
        "\n",
        "Dense prediction requires a network to understand the whole scene and still deliver pixel-level answers. Encoder–decoder architectures resolve the tension between global semantics and local detail by compressing, reasoning, and then expanding feature maps with careful skip connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f2c27d38",
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using repo root: /media/abdul-aziz/sdb7/masters_research/math_course_dlcv\n"
          ]
        }
      ],
      "source": [
        "# Ensure repo root is on sys.path for local imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if not (repo_root / 'days').exists():\n",
        "    # Walk upward to find the repo root\n",
        "    for parent in Path.cwd().resolve().parents:\n",
        "        if (parent / 'days').exists():\n",
        "            repo_root = parent\n",
        "            break\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print(f'Using repo root: {repo_root}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973efe7d",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Core Intuition — From Classification to Dense Prediction\n",
        "\n",
        "- Classification answers *what* is present; dense prediction answers *what is at every pixel*.\n",
        "- Deep layers capture meaning but lose detail, while shallow layers keep detail but lack meaning.\n",
        "- Successful dense predictors combine deep semantic context with shallow spatial precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "856cb006",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Encoder–Decoder Blueprint\n",
        "\n",
        "**Encoder (down path)**\n",
        "\n",
        "- Convolutions + stride/pooling reduce resolution.\n",
        "- Receptive field grows, semantics strengthen.\n",
        "\n",
        "**Decoder (up path)**\n",
        "\n",
        "- Upsampling or transposed convolutions recover spatial resolution.\n",
        "- Skip connections inject spatial clues so localization remains sharp.\n",
        "\n",
        "> Compress → understand → expand → localize. This template appears in UNet, FPN, SegNet, DeepLab decoders, diffusion U-Nets, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd834ef",
      "metadata": {},
      "source": [
        "\n",
        "## 3. UNet — Precise Localization via Skip Connections\n",
        "\n",
        "- Originated in medical imaging where each pixel matters.\n",
        "- Concatenates shallow encoder features with decoder features at the same scale.\n",
        "- Preserves boundary sharpness, works with limited data, and keeps gradients short.\n",
        "- Remains the backbone for medical segmentation, satellite change detection, and diffusion models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b78099",
      "metadata": {},
      "source": [
        "\n",
        "## 4. FPN — Multi-Scale Feature Fusion\n",
        "\n",
        "- Objects appear at many scales. FPN fuses features from every backbone stage.\n",
        "- Bottom-up CNN produces \\(C_2, C_3, C_4, C_5\\). Top-down pathway upsamples and adds lateral skips to yield \\(P_2, P_3, P_4, P_5\\).\n",
        "- Results in high-resolution, high-semantic maps ready for detection and instance segmentation heads (Faster/Mask R-CNN, RetinaNet, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474a516a",
      "metadata": {},
      "source": [
        "\n",
        "## 5. UNet vs FPN — Design Differences\n",
        "\n",
        "| Aspect | UNet | FPN |\n",
        "| --- | --- | --- |\n",
        "| Primary task | Semantic segmentation | Detection & instance segmentation |\n",
        "| Skip strategy | Concatenate encoder + decoder maps | Add lateral features | \n",
        "| Decoder depth | Symmetric, full decoder | Lightweight top-down pyramid |\n",
        "| Spatial precision | Very high | High |\n",
        "| Semantic consistency | Medium | Very strong |\n",
        "| Typical domains | Medical, remote sensing, diffusion | Detection, panoptic segmentation |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603fb36a",
      "metadata": {},
      "source": [
        "\n",
        "## 6. Upsampling & Transposed Conv Intuition\n",
        "\n",
        "- Nearest/bilinear interpolation: fast baselines with no learnable weights.\n",
        "- Transposed convolution: learned upsampling (the gradient of convolution w.r.t. its input); powerful but needs care to avoid checkerboard artifacts.\n",
        "- Resize + convolution blocks are now common for stable, expressive decoders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e80bbc",
      "metadata": {},
      "source": [
        "\n",
        "## 7. Encoder–Decoder Gradient Flow\n",
        "\n",
        "Skip connections shorten gradient paths, decoders send localization feedback to encoders, and supervision at full resolution keeps learning stable. This is why UNet-style models converge quickly even on small datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de719ee2",
      "metadata": {},
      "source": [
        "\n",
        "## 8. Applications\n",
        "\n",
        "- Semantic segmentation (UNet, DeepLab-UNet, HRNet decoders).\n",
        "- Object detection & instance segmentation (ResNet+FPN, ConvNeXt+FPN).\n",
        "- Change detection and depth/surface prediction.\n",
        "- Diffusion models and autoencoders rely on UNet variants for denoising.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "550fc074",
      "metadata": {},
      "source": [
        "\n",
        "## 9. Python — Blueprint Summaries\n",
        "\n",
        "`days/day21/code/encoder_decoder.py` lists representative encoder–decoder families.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bcb0a66b",
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet: Symmetric encoder–decoder with spatial skip concatenations skip → Concatenate shallow + deep feature maps excels at → Pixel-precise segmentation and diffusion backbones\n",
            "FPN: Top-down pyramid that fuses multi-scale backbone features skip → Add lateral features across resolutions excels at → Detection/instance segmentation needing multi-scale context\n",
            "SegNet: Encoder–decoder that remembers pooling indices skip → Unpool using saved max-pooling masks excels at → Semantic segmentation when memory is limited\n"
          ]
        }
      ],
      "source": [
        "from days.day21.code.encoder_decoder import summarize_blueprints\n",
        "\n",
        "for bp in summarize_blueprints():\n",
        "  print(f\"{bp.name}: {bp.description} skip → {bp.skip_strategy} excels at → {bp.excels_at}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72aa45d",
      "metadata": {},
      "source": [
        "\n",
        "## 10. Visualization — Encoder–Decoder + FPN Diagram\n",
        "\n",
        "`days/day21/code/visualizations.py` renders a simple schematic comparing UNet-style skip concatenation with the FPN pyramid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "af220874",
      "metadata": {
        "tags": [
          "python"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RUN_FIGURES = True to regenerate Day 21 figures inside days/day21/outputs/.\n"
          ]
        }
      ],
      "source": [
        "from days.day21.code.visualizations import create_encoder_decoder_diagram\n",
        "\n",
        "RUN_FIGURES = False\n",
        "\n",
        "if RUN_FIGURES:\n",
        "    path = create_encoder_decoder_diagram()\n",
        "    path\n",
        "else:\n",
        "    print(\"Set RUN_FIGURES = True to regenerate Day 21 figures inside days/day21/outputs/.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5da9c7",
      "metadata": {},
      "source": [
        "\n",
        "## 11. Mini Exercises\n",
        "\n",
        "1. Implement a tiny UNet and visualize encoder/decoder feature maps.\n",
        "2. Replace concatenation with addition in UNet skips and inspect boundary quality.\n",
        "3. Build an FPN on top of ResNet-50 and compare detection performance vs. a plain head.\n",
        "4. Compare bilinear vs. transposed convolution upsampling artifacts.\n",
        "5. Train a Siamese UNet for change detection on satellite imagery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ff28d8",
      "metadata": {},
      "source": [
        "\n",
        "## 12. Key Takeaways\n",
        "\n",
        "- Dense prediction needs both global context and local precision.\n",
        "- Encoder–decoder blueprints achieve this by compressing then expanding with skip connections.\n",
        "- UNet prioritizes pixel-accurate segmentation; FPN prioritizes multi-scale semantics.\n",
        "- Upsampling choices and skip designs shape gradient flow and final quality.\n",
        "- These ideas power modern segmentation, detection, depth, and diffusion models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
